{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgRnlyH58OIV"
      },
      "source": [
        "# Logistic Regression Gradient & Convexity — Student Lab\n",
        "\n",
        "Complete all TODOs. This lab is math-first and stability-first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UsPsK8nl8OIW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9Yz6Ep8OIW"
      },
      "source": [
        "## Section 0 — Synthetic Dataset\n",
        "We’ll create a binary classification dataset with both separable and non-separable regimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvsr2O9v8OIW",
        "outputId": "3eeb4872-acde-4918-b7a5-66219c157bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 5), np.float64(0.515))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "def make_data(n=400, d=5, separable=False):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    w_true = rng.standard_normal(d)\n",
        "    logits = X @ w_true\n",
        "    if separable:\n",
        "        y = (logits > 0).astype(int)\n",
        "    else:\n",
        "        # add noise so it's not perfectly separable\n",
        "        logits = logits + 0.5 * rng.standard_normal(n)\n",
        "        probs = 1 / (1 + np.exp(-logits))\n",
        "        y = (rng.random(n) < probs).astype(int)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_data(separable=False)\n",
        "check('shapes', X.ndim==2 and y.ndim==1 and X.shape[0]==y.shape[0])\n",
        "X.shape, y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dG6cvjX8OIW"
      },
      "source": [
        "## Section 1 — Sigmoid + Stable Loss\n",
        "\n",
        "### Task 1.1: Stable sigmoid\n",
        "\n",
        "# TODO: implement a numerically stable sigmoid.\n",
        "# HINT:\n",
        "# - Use `np.where(z>=0, ...)` trick to avoid overflow.\n",
        "\n",
        "**Explain:** Why does sigmoid saturate for large |z| and what does that do to gradients?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGuKn_jP8OIX",
        "outputId": "c263291b-40f2-49e4-aeae-6b43b14f43f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00000000e+00 4.53978687e-05 5.00000000e-01 9.99954602e-01\n",
            " 1.00000000e+00]\n",
            "OK: in_0_1\n",
            "OK: monotonic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1819154982.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  x = 1 / (1 + np.exp(-z))\n",
            "/tmp/ipython-input-1819154982.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  y = np.exp(z) / (1 + np.exp(z))\n",
            "/tmp/ipython-input-1819154982.py:5: RuntimeWarning: invalid value encountered in divide\n",
            "  y = np.exp(z) / (1 + np.exp(z))\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    # TODO\n",
        "    z = np.asarray(z)\n",
        "    x = 1 / (1 + np.exp(-z))\n",
        "    y = np.exp(z) / (1 + np.exp(z))\n",
        "    return np.where(z >= 0, x, y)\n",
        "\n",
        "z = np.array([-1000.0, -10.0, 0.0, 10.0, 1000.0])\n",
        "p = sigmoid(z)\n",
        "print(p)\n",
        "check('in_0_1', np.all((p >= 0) & (p <= 1)))\n",
        "check('monotonic', np.all(np.diff(p) >= 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SagXvQu58OIX"
      },
      "source": [
        "### Task 1.2: Stable binary cross-entropy loss\n",
        "\n",
        "We use labels y in {0,1}.\n",
        "\n",
        "Loss per example: `-y log(p) - (1-y) log(1-p)` where p=sigmoid(z).\n",
        "\n",
        "# TODO: implement stable loss without NaNs/inf.\n",
        "# HINT:\n",
        "# - Clip p to [eps, 1-eps]\n",
        "# - Alternatively use softplus: log(1+exp(z))\n",
        "\n",
        "**Interview Angle:** explain a stable form of log-loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QROiZR6-8OIX",
        "outputId": "eaa816ae-69a7-4f7f-f6ad-7381a24072be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.6931471805599452\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def bce_loss(y, p):\n",
        "    # TODO\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    y = y.astype(float)\n",
        "    return float(-np.mean(y * np.log(p) + (1 - y)*np.log(1 - p)))\n",
        "\n",
        "# sanity: loss is finite\n",
        "z = X @ np.zeros(X.shape[1])\n",
        "p = sigmoid(z)\n",
        "L = bce_loss(y, p)\n",
        "print('loss', L)\n",
        "check('finite_loss', np.isfinite(L))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPogTBnp8OIX"
      },
      "source": [
        "## Section 2 — Gradient (Derive → Implement → Check)\n",
        "\n",
        "### Task 2.1: Derive the gradient (write in markdown)\n",
        "Show that for loss averaged over n examples:\n",
        "`grad = X^T (p - y) / n`\n",
        "\n",
        "**Checkpoint:** What is the shape of grad?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49WFzlMs8OIX"
      },
      "source": [
        "### Task 2.2: Implement loss and gradient\n",
        "\n",
        "# TODO: implement `logreg_loss_and_grad(X, y, w)` returning (loss, grad).\n",
        "# HINT:\n",
        "# - z = X @ w\n",
        "# - p = sigmoid(z)\n",
        "# - loss = BCE\n",
        "# - grad = X.T @ (p - y) / n\n",
        "\n",
        "**FAANG gotcha:** ensure y is 0/1, not -1/+1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvaHEn-8OIX",
        "outputId": "20f5f7db-842e-41bb-8666-7e44488486d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 1.0600908718928417 grad_norm 0.4480760385916178\n",
            "OK: grad_shape\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def logreg_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    z = X @ w\n",
        "    # p = 1 / (1 + np.exp(-z))\n",
        "    p = sigmoid(z)\n",
        "    loss = float(-np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
        "    n = X.shape[0]\n",
        "    grad = X.T @ (p - y) / n\n",
        "    return loss, grad\n",
        "\n",
        "w0 = rng.standard_normal(X.shape[1])\n",
        "loss, grad = logreg_loss_and_grad(X, y, w0)\n",
        "print('loss', loss, 'grad_norm', np.linalg.norm(grad))\n",
        "check('grad_shape', grad.shape == w0.shape)\n",
        "check('finite', np.isfinite(loss) and np.all(np.isfinite(grad)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaIPMqf_8OIX"
      },
      "source": [
        "### Task 2.3: Numerical gradient check (finite differences)\n",
        "\n",
        "# TODO: implement numerical gradient and compare to analytic grad.\n",
        "# HINT: central difference\n",
        "\n",
        "**Checkpoint:** Why can eps too small make the check worse?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-ze0-A18OIY"
      },
      "outputs": [],
      "source": [
        "def numerical_grad(f, w, eps=1e-5):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "f = lambda v: logreg_loss_and_grad(X, y, v)[0]\n",
        "g_num = numerical_grad(f, w0)\n",
        "_, g = logreg_loss_and_grad(X, y, w0)\n",
        "\n",
        "max_abs = np.max(np.abs(g - g_num))\n",
        "print('max_abs_diff', max_abs)\n",
        "check('grad_check', max_abs < 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZWcuwyY8OIY"
      },
      "source": [
        "## Section 3 — Convexity & Hessian Intuition\n",
        "\n",
        "### Task 3.1: Hessian-vector product (HVP)\n",
        "\n",
        "For logistic regression:\n",
        "H = (1/n) X^T S X where S = diag(p(1-p)).\n",
        "\n",
        "Implement HVP: compute H@v without building full H explicitly.\n",
        "\n",
        "# HINT:\n",
        "- s = p*(1-p)\n",
        "- compute Xv then multiply by s then X^T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udD0AsSW8OIY"
      },
      "outputs": [],
      "source": [
        "def hvp(X, y, w, v):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "v = rng.standard_normal(X.shape[1])\n",
        "Hv = hvp(X, y, w0, v)\n",
        "check('hvp_shape', Hv.shape == v.shape)\n",
        "check('hvp_finite', np.all(np.isfinite(Hv)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL1x98KX8OIY"
      },
      "source": [
        "### Task 3.2: Empirical PSD check\n",
        "\n",
        "Check that v^T H v >= 0 for random v (PSD).\n",
        "\n",
        "**Explain:** Why does PSD imply convexity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imAxVIlH8OIY"
      },
      "outputs": [],
      "source": [
        "def vHv(X, y, w, v):\n",
        "    return float(v @ hvp(X, y, w, v))\n",
        "\n",
        "vals = []\n",
        "for _ in range(50):\n",
        "    v = rng.standard_normal(X.shape[1])\n",
        "    vals.append(vHv(X, y, w0, v))\n",
        "print('min vHv', min(vals))\n",
        "check('psd', min(vals) > -1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4feikQ8OIY"
      },
      "source": [
        "## Section 4 — Optimization (Bonus)\n",
        "\n",
        "### Task 4.1: One step of GD vs Newton (conceptual)\n",
        "\n",
        "Implement one gradient descent step. (Newton step is optional.)\n",
        "\n",
        "**FAANG gotcha:** perfectly separable data can push weights to infinity; explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXGwPWbF8OIY"
      },
      "outputs": [],
      "source": [
        "def gd_step(X, y, w, lr):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "w1 = gd_step(X, y, w0, lr=0.1)\n",
        "loss0, _ = logreg_loss_and_grad(X, y, w0)\n",
        "loss1, _ = logreg_loss_and_grad(X, y, w1)\n",
        "print('loss0', loss0, 'loss1', loss1)\n",
        "check('decreased', loss1 <= loss0 + 1e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k66W4EWa8OIY"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Gradient check passes\n",
        "- PSD check passes\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ]
}