{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgRnlyH58OIV"
      },
      "source": [
        "# Logistic Regression Gradient & Convexity — Student Lab\n",
        "\n",
        "Complete all TODOs. This lab is math-first and stability-first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UsPsK8nl8OIW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9Yz6Ep8OIW"
      },
      "source": [
        "## Section 0 — Synthetic Dataset\n",
        "We’ll create a binary classification dataset with both separable and non-separable regimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvsr2O9v8OIW",
        "outputId": "7c73f136-d447-4e79-fb6e-18ac371f496b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 5), np.float64(0.515))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "def make_data(n=400, d=5, separable=False):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    w_true = rng.standard_normal(d)\n",
        "    logits = X @ w_true\n",
        "    if separable:\n",
        "        y = (logits > 0).astype(int)\n",
        "    else:\n",
        "        # add noise so it's not perfectly separable\n",
        "        logits = logits + 0.5 * rng.standard_normal(n)\n",
        "        probs = 1 / (1 + np.exp(-logits))\n",
        "        y = (rng.random(n) < probs).astype(int)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_data(separable=False)\n",
        "check('shapes', X.ndim==2 and y.ndim==1 and X.shape[0]==y.shape[0])\n",
        "X.shape, y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dG6cvjX8OIW"
      },
      "source": [
        "## Section 1 — Sigmoid + Stable Loss\n",
        "\n",
        "### Task 1.1: Stable sigmoid\n",
        "\n",
        "# TODO: implement a numerically stable sigmoid.\n",
        "# HINT:\n",
        "# - Use `np.where(z>=0, ...)` trick to avoid overflow.\n",
        "\n",
        "**Explain:** Why does sigmoid saturate for large |z| and what does that do to gradients?\n",
        "\n",
        "**Answer:** The sigmoid function squeezes any number into a value between 0 and 1. When the input z is very large and positive, sigmoid becomes almost 1. When z is very large and negative, sigmoid becomes almost 0. Beyond a point, increasing or decreasing z hardly changes the output; this is called saturation.\n",
        "\n",
        "Because the sigmoid curve becomes almost flat in these regions, its slope (gradient) becomes very close to zero. That means the model gets almost no learning signal: changing the weights doesn’t change the output much, so training slows down or can even stall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGuKn_jP8OIX",
        "outputId": "336ea354-ec3e-47a2-de96-28d6946ccce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00000000e+00 4.53978687e-05 5.00000000e-01 9.99954602e-01\n",
            " 1.00000000e+00]\n",
            "OK: in_0_1\n",
            "OK: monotonic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1819154982.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  x = 1 / (1 + np.exp(-z))\n",
            "/tmp/ipython-input-1819154982.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  y = np.exp(z) / (1 + np.exp(z))\n",
            "/tmp/ipython-input-1819154982.py:5: RuntimeWarning: invalid value encountered in divide\n",
            "  y = np.exp(z) / (1 + np.exp(z))\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    # TODO\n",
        "    z = np.asarray(z)\n",
        "    x = 1 / (1 + np.exp(-z))\n",
        "    y = np.exp(z) / (1 + np.exp(z))\n",
        "    return np.where(z >= 0, x, y)\n",
        "\n",
        "z = np.array([-1000.0, -10.0, 0.0, 10.0, 1000.0])\n",
        "p = sigmoid(z)\n",
        "print(p)\n",
        "check('in_0_1', np.all((p >= 0) & (p <= 1)))\n",
        "check('monotonic', np.all(np.diff(p) >= 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SagXvQu58OIX"
      },
      "source": [
        "### Task 1.2: Stable binary cross-entropy loss\n",
        "\n",
        "We use labels y in {0,1}.\n",
        "\n",
        "Loss per example: `-y log(p) - (1-y) log(1-p)` where p=sigmoid(z).\n",
        "\n",
        "# TODO: implement stable loss without NaNs/inf.\n",
        "# HINT:\n",
        "# - Clip p to [eps, 1-eps]\n",
        "# - Alternatively use softplus: log(1+exp(z))\n",
        "\n",
        "**Interview Angle:** explain a stable form of log-loss.\n",
        "\n",
        "**Answer:** Clipping works, but the “clean” stable approach is to compute BCE from logits z directly, avoiding sigmoid + logs.\n",
        "For numerical stability, don’t compute BCE using log(sigmoid(z)) directly. Use a logits-based form with softplus (like softplus(z) - y*z), which avoids log(0) and overflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QROiZR6-8OIX",
        "outputId": "41ec8d69-f12a-4fe8-cfee-89d49d1cdb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.6931471805599452\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def bce_loss(y, p):\n",
        "    # TODO\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    y = y.astype(float)\n",
        "    return float(-np.mean(y * np.log(p) + (1 - y)*np.log(1 - p)))\n",
        "\n",
        "# sanity: loss is finite\n",
        "z = X @ np.zeros(X.shape[1])\n",
        "p = sigmoid(z)\n",
        "L = bce_loss(y, p)\n",
        "print('loss', L)\n",
        "check('finite_loss', np.isfinite(L))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPogTBnp8OIX"
      },
      "source": [
        "## Section 2 — Gradient (Derive → Implement → Check)\n",
        "\n",
        "### Task 2.1: Derive the gradient (write in markdown)\n",
        "Show that for loss averaged over n examples:\n",
        "`grad = X^T (p - y) / n`\n",
        "\n",
        "**Checkpoint:** What is the shape of grad?\n",
        "\n",
        "**Answer:** The gradient has the same shape as the weight vector w."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49WFzlMs8OIX"
      },
      "source": [
        "### Task 2.2: Implement loss and gradient\n",
        "\n",
        "# TODO: implement `logreg_loss_and_grad(X, y, w)` returning (loss, grad).\n",
        "# HINT:\n",
        "# - z = X @ w\n",
        "# - p = sigmoid(z)\n",
        "# - loss = BCE\n",
        "# - grad = X.T @ (p - y) / n\n",
        "\n",
        "**FAANG gotcha:** ensure y is 0/1, not -1/+1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvaHEn-8OIX",
        "outputId": "911710cb-11a7-420e-cdb9-1a1f670b5fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 1.3053171072371215 grad_norm 0.5956359579815794\n",
            "OK: grad_shape\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def logreg_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    z = X @ w\n",
        "    # p = 1 / (1 + np.exp(-z))\n",
        "    p = sigmoid(z) # using this one since it is more numerically stable\n",
        "    loss = float(-np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)))\n",
        "    n = X.shape[0]\n",
        "    grad = X.T @ (p - y) / n\n",
        "    return loss, grad\n",
        "\n",
        "w0 = rng.standard_normal(X.shape[1])\n",
        "loss, grad = logreg_loss_and_grad(X, y, w0)\n",
        "print('loss', loss, 'grad_norm', np.linalg.norm(grad))\n",
        "check('grad_shape', grad.shape == w0.shape)\n",
        "check('finite', np.isfinite(loss) and np.all(np.isfinite(grad)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaIPMqf_8OIX"
      },
      "source": [
        "### Task 2.3: Numerical gradient check (finite differences)\n",
        "\n",
        "# TODO: implement numerical gradient and compare to analytic grad.\n",
        "# HINT: central difference\n",
        "\n",
        "**Checkpoint:** Why can eps too small make the check worse?\n",
        "\n",
        "**Answer:** Because of floating-point precision.\n",
        "\n",
        "When eps is extremely tiny:\n",
        "- f(w + epsilon) and f(w - epsilon) become almost identical\n",
        "- Subtracting them causes you to lose meaningful digits\n",
        "- The numerator becomes dominated by rounding noise\n",
        "- Then dividing by 2\\epsilon can amplify that noise\n",
        "\n",
        "So paradoxically: smaller eps does not always mean more accurate.\n",
        "\n",
        "In short numerical gradient checking works by measuring loss change from tiny weight nudges; if eps is too small, floating-point rounding makes the difference noisy, so the gradient estimate gets worse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-ze0-A18OIY",
        "outputId": "47bf4d4d-705e-418b-e6e8-8a2ec0a3f83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_abs_diff 2.187561243260916e-11\n",
            "OK: grad_check\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad(f, w, eps=1e-5):\n",
        "    # TODO\n",
        "    w = np.asarray(w, dtype=float)\n",
        "    g = np.zeros_like(w)\n",
        "    for i in range(w.size):\n",
        "      e = np.zeros_like(w); e[i] = 1.0\n",
        "      g[i] = (f(w + eps * e) - f(w - eps * e)) / (2 * eps)\n",
        "    return g\n",
        "\n",
        "f = lambda v: logreg_loss_and_grad(X, y, v)[0]\n",
        "g_num = numerical_grad(f, w0)\n",
        "_, g = logreg_loss_and_grad(X, y, w0)\n",
        "\n",
        "max_abs = np.max(np.abs(g - g_num))\n",
        "print('max_abs_diff', max_abs)\n",
        "check('grad_check', max_abs < 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZWcuwyY8OIY"
      },
      "source": [
        "## Section 3 — Convexity & Hessian Intuition\n",
        "\n",
        "### Task 3.1: Hessian-vector product (HVP)\n",
        "\n",
        "For logistic regression:\n",
        "H = (1/n) X^T S X where S = diag(p(1-p)).\n",
        "\n",
        "Implement HVP: compute H@v without building full H explicitly.\n",
        "\n",
        "# HINT:\n",
        "- s = p*(1-p)\n",
        "- compute Xv then multiply by s then X^T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udD0AsSW8OIY",
        "outputId": "26ec0b00-7b39-43ee-f718-f1684548a70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: hvp_shape\n",
            "OK: hvp_finite\n"
          ]
        }
      ],
      "source": [
        "def hvp(X, y, w, v):\n",
        "    # TODO\n",
        "    z = X @ w\n",
        "    p = sigmoid(z)\n",
        "    s = p * (1 - p)\n",
        "    Xv = X @ v\n",
        "    return (X.T @ (s * Xv)) / X.shape[0]\n",
        "\n",
        "v = rng.standard_normal(X.shape[1])\n",
        "Hv = hvp(X, y, w0, v)\n",
        "check('hvp_shape', Hv.shape == v.shape)\n",
        "check('hvp_finite', np.all(np.isfinite(Hv)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL1x98KX8OIY"
      },
      "source": [
        "### Task 3.2: Empirical PSD check\n",
        "\n",
        "Check that v^T H v >= 0 for random v (PSD).\n",
        "\n",
        "**Explain:** Why does PSD imply convexity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imAxVIlH8OIY",
        "outputId": "128a4fa8-889e-4d77-acee-ee6106b962f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min vHv 0.10348656174059763\n",
            "OK: psd\n"
          ]
        }
      ],
      "source": [
        "def vHv(X, y, w, v):\n",
        "    return float(v @ hvp(X, y, w, v))\n",
        "\n",
        "vals = []\n",
        "for _ in range(50):\n",
        "    v = rng.standard_normal(X.shape[1])\n",
        "    vals.append(vHv(X, y, w0, v))\n",
        "print('min vHv', min(vals))\n",
        "check('psd', min(vals) > -1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4feikQ8OIY"
      },
      "source": [
        "## Section 4 — Optimization (Bonus)\n",
        "\n",
        "### Task 4.1: One step of GD vs Newton (conceptual)\n",
        "\n",
        "Implement one gradient descent step. (Newton step is optional.)\n",
        "\n",
        "**FAANG gotcha:** perfectly separable data can push weights to infinity; explain why.\n",
        "\n",
        "**Answer:** Perfectly separable data means that you can draw a line that separates the classes with zero error.\n",
        "\n",
        "In logistic regression:\n",
        "- The model can always reduce loss by making predictions more confident\n",
        "- More confidence = larger weights\n",
        "- There is no penalty for weights becoming huge (unless regularized)\n",
        "\n",
        "So the optimizer keeps doing this: Make weights bigger -> predictions closer to 0 or 1 -> loss keeps decreasing\n",
        "\n",
        "In short gradient descent reduces loss by stepping opposite the gradient, but on perfectly separable data, logistic regression has no finite optimum, so the optimizer keeps pushing weights toward infinity unless regularization is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXGwPWbF8OIY",
        "outputId": "7a373bee-71d7-4d24-9800-02a8b00ab2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0 1.3053171072371215 loss1 1.2700539510700066\n",
            "OK: decreased\n"
          ]
        }
      ],
      "source": [
        "def gd_step(X, y, w, lr):\n",
        "    # TODO\n",
        "    _, g = logreg_loss_and_grad(X, y, w)\n",
        "    return w - lr * g\n",
        "\n",
        "w1 = gd_step(X, y, w0, lr=0.1)\n",
        "loss0, _ = logreg_loss_and_grad(X, y, w0)\n",
        "loss1, _ = logreg_loss_and_grad(X, y, w1)\n",
        "print('loss0', loss0, 'loss1', loss1)\n",
        "check('decreased', loss1 <= loss0 + 1e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k66W4EWa8OIY"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Gradient check passes\n",
        "- PSD check passes\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ]
}